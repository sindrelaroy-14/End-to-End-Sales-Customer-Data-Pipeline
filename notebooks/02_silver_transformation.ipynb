{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb339313-aefa-49cb-a935-eca9fafe42fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, trim, current_timestamp, row_number, desc, lit, when\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "from functools import reduce\n",
    "\n",
    "def transform_to_silver(entity_name, primary_key):\n",
    "    # 1. Paths and Table Names\n",
    "    bronze_path = f\"/Volumes/sales_project/bronze/landing_zone_metadata/data/{entity_name}\"\n",
    "    \n",
    "    if entity_name in [\"orders\", \"order_items\"]:\n",
    "        silver_table_name = f\"sales_project.silver.dim_{entity_name}\"\n",
    "        history_table_name = f\"sales_project.silver.history_dim_{entity_name}\"\n",
    "    else:\n",
    "        silver_table_name = f\"sales_project.silver.fact_{entity_name}\"\n",
    "        history_table_name = f\"sales_project.silver.history_fact_{entity_name}\"\n",
    "\n",
    "    print(f\"Processing: {entity_name} -> {silver_table_name}\")\n",
    "\n",
    "    # 2. Read and DQM (Data Quality Management)\n",
    "    df_bronze = spark.read.format(\"delta\").load(bronze_path)\n",
    "    \n",
    "    # DQM: Trim strings\n",
    "    for column in df_bronze.columns:\n",
    "        if dict(df_bronze.dtypes)[column] == \"string\":\n",
    "            df_bronze = df_bronze.withColumn(column, trim(col(column)))\n",
    "    \n",
    "    # DQM: Lowercase email\n",
    "    if \"email\" in df_bronze.columns:\n",
    "        df_bronze = df_bronze.withColumn(\"email\", lower(col(\"email\")))\n",
    "        \n",
    "    # DQM: Remove rows with null or empty Primary Keys\n",
    "    df_bronze = df_bronze.filter((col(primary_key).isNotNull()) & (col(primary_key) != \"\"))\n",
    "\n",
    "    # Deduplication: Ensure only the latest record per PK from Bronze is processed\n",
    "    window_spec = Window.partitionBy(primary_key).orderBy(desc(\"ingested_at\"))\n",
    "    df_bronze_latest = df_bronze.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "                                .filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "    # 3. Day 1: Initialize Silver Table\n",
    "    if not spark.catalog.tableExists(silver_table_name):\n",
    "        print(f\"Day 1 load for {entity_name}\")\n",
    "        (df_bronze_latest\n",
    "         .withColumn(\"active_flag\", lit(\"Y\"))\n",
    "         .withColumn(\"change_type\", lit(\"NC\"))\n",
    "         .withColumn(\"silver_updated_at\", current_timestamp())\n",
    "         .write.format(\"delta\").saveAsTable(silver_table_name))\n",
    "        return\n",
    "\n",
    "    # 4. Day 2+: Compare Silver and Bronze\n",
    "    df_silver = spark.table(silver_table_name)\n",
    "\n",
    "    df_join = df_bronze_latest.alias(\"b\").join(\n",
    "        df_silver.alias(\"s\"),\n",
    "        col(f\"b.{primary_key}\") == col(f\"s.{primary_key}\"),\n",
    "        \"outer\"\n",
    "    )\n",
    "\n",
    "    # Compare columns for Updates (Excluding technical/metadata columns)\n",
    "    cols_to_compare = [c for c in df_bronze_latest.columns if c != primary_key]\n",
    "    compare_exprs = [col(f\"b.{c}\") != col(f\"s.{c}\") for c in cols_to_compare]\n",
    "\n",
    "    # Resolve ambiguity and define Change Type\n",
    "    df_changes = df_join.select(\n",
    "        *[when(col(f\"b.{c}\").isNotNull(), col(f\"b.{c}\")).otherwise(col(f\"s.{c}\")).alias(c) for c in df_bronze_latest.columns],\n",
    "        when(col(f\"s.{primary_key}\").isNull(), lit(\"I\"))\n",
    "        .when(col(f\"b.{primary_key}\").isNull(), lit(\"D\"))\n",
    "        .when(reduce(lambda a, b: a | b, compare_exprs), lit(\"U\"))\n",
    "        .otherwise(lit(\"NC\")).alias(\"change_type\")\n",
    "    )\n",
    "\n",
    "    # 5. Archive Logic (Move to History)\n",
    "    # Get current Silver records for items that are being Updated (U) or Deleted (D)\n",
    "    df_to_history = df_silver.alias(\"old_s\").join(\n",
    "        df_changes.filter(col(\"change_type\").isin(\"U\", \"D\")).select(primary_key),\n",
    "        primary_key\n",
    "    )\n",
    "\n",
    "    # Update flags for History: N for Updates, D for Deletes\n",
    "    df_history_final = df_to_history.withColumn(\n",
    "        \"active_flag\", when(col(\"change_type\") == \"U\", lit(\"N\")).otherwise(lit(\"D\"))\n",
    "    ).withColumn(\"archived_at\", current_timestamp())\n",
    "\n",
    "    if not df_history_final.isEmpty():\n",
    "        df_history_final.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(history_table_name)\n",
    "\n",
    "    # 6. Update Silver Table (Current Snapshot)\n",
    "    silver_table = DeltaTable.forName(spark, silver_table_name)\n",
    "    \n",
    "    # Physically remove Deletes (D) and Old versions of Updates (U) from Silver\n",
    "    ids_to_remove = [row[primary_key] for row in df_changes.filter(col(\"change_type\").isin(\"U\", \"D\")).select(primary_key).collect()]\n",
    "    if ids_to_remove:\n",
    "        silver_table.delete(col(primary_key).isin(ids_to_remove))\n",
    "\n",
    "    # Insert New (I) and New versions of Updates (U) back into Silver as Active\n",
    "    df_to_upsert = df_changes.filter(col(\"change_type\").isin(\"I\", \"U\")) \\\n",
    "                             .withColumn(\"active_flag\", lit(\"Y\")) \\\n",
    "                             .withColumn(\"silver_updated_at\", current_timestamp())\n",
    "\n",
    "    if not df_to_upsert.isEmpty():\n",
    "        df_to_upsert.write.format(\"delta\").mode(\"append\").saveAsTable(silver_table_name)\n",
    "\n",
    "    print(f\"Success: {entity_name} processed according to Day 2 logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2737f3f-ac35-403b-aa38-cc39909ec598",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "# List of tables with their name and primary keys \n",
    "entity_list = [\n",
    "    {\"name\" : \"customers\", \"pk\" : \"customer_id\"},\n",
    "    {\"name\" : \"products\", \"pk\" : \"product_id\"},\n",
    "    {\"name\" : \"orders\", \"pk\" : \"order_id\"},\n",
    "    {\"name\" : \"order_items\", \"pk\" : \"order_item_id\"}\n",
    "]\n",
    "\n",
    "# Initialize the Silver Schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS sales_project.silver\")\n",
    "\n",
    "# Run the pipeline\n",
    "for entity in entity_list:\n",
    "    transform_to_silver(entity[\"name\"], entity[\"pk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3afad0b-d472-4168-a392-cbb53f81fdd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"sales_project.silver.fact_customers\"))\n",
    "# You should see all records with active_flag = 'Y' and change_type = 'NC'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
